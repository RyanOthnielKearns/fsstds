{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data to a DataFrame\n",
    "Thank you for checking out the code for: \n",
    "\n",
    "> Hogan, Bernie (2022, forthcoming) _From Social Science to Data Science_. Sage Publications. \n",
    "\n",
    "This notebook contains the code from the book, along with the headers and additional author notes that are not in the book as a way to help navigate the code. You can run this notebook in a browser by clicking the buttons below. \n",
    "    \n",
    "The version that is uploaded to GitHub should have all the results pasted, but the best way to follow along is to clear all outputs and then start afresh. To do this in Jupyter go the menu and select \"Kernel -> Restart Kernel and Clear all Outputs...\". To do this on Google Colab go to the menu and select \"Edit -> Clear all outputs\".\n",
    "    \n",
    "The most up-to-date version of this code can be found at https://www.github.com/berniehogan/fsstds \n",
    "\n",
    "Additional resources and teaching materials can be found on Sage's forthcoming website for this book. \n",
    "\n",
    "All code for the book and derivative code on the book's repository is released open source under the  MIT license. \n",
    "    \n",
    "\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/berniehogan/fsstds/main?filepath=chapters%2FCh.04.FileTypes.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/berniehogan/fsstds/blob/main/chapters/Ch.04.FileTypes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A important note on file organisation\n",
    "\n",
    "Excerpt from the book:\n",
    "\n",
    "> \"Starting with this chapter I will be making use of existing data. This data is available on the webpage for this book, even if it is drawn from publicly available sources. You should store this data in a folder adjacent to the folder where you store your Jupyter notebooks, not in the same folder as the notebooks. This keeps your files and your data much more organised. I have opted to do that with the code below. So if this file is in a folder called '`chapters`' under a folder called `'book'`, then you should also have a folder called `'data'` under `'book'`, like so: \n",
    "\n",
    "~~~\n",
    "book\n",
    " |- chapters\n",
    " |- data\n",
    " |- exercises\n",
    " |- etc.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory can be found at: /Users/work/Documents/GitHub/FSSDS/book/data.\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path.cwd().parent / \"data\"\n",
    "\n",
    "try:\n",
    "    if not data_dir.exists(): data_dir.mkdir()\n",
    "except:\n",
    "    print(f\"There was an issue creating the directory at {data_dir}\")\n",
    "else:\n",
    "    print(f\"The data directory can be found at: {data_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data \n",
    "\n",
    "Excerpt from the book: \n",
    "> \"This chapter uses example data from places online where the data is freely available for secondary use. These will be available in the GitHub repository for this book. The path for the data folder is https://github.com/berniehogan/fsstds/tree/main/data . From here you can download the example files one by one. If you \"clone\" the repository and you have a copy of GitHub desktop (or know how to use GitHub from the command line) you can then copy the entire repository, which will include the data as well as all the notebooks with the paths in the correct place. To remind, these notebooks primarily contain the code. The book itself is available through Sage.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectangular data: CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `csv` library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Gender', 'Species', 'Appearance']\n",
      "['Fozzie', 'Male', 'Bear', '1976']\n",
      "['Kermit', 'Male', 'Frog', '1955']\n",
      "['Piggy', 'Female', 'Pig', '1974']\n",
      "['Gonzo', 'Male', '', '1970']\n",
      "['Rowlf', 'Male', 'Dog', '1962']\n",
      "['Beaker', '', 'Muppet', '1977']\n",
      "['Janice', 'Female', 'Muppet', '1975']\n",
      "['Hilda', 'Female', 'Muppet', '1976']\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir / \"MuppetsTable_simple.csv\") as filein:\n",
    "    file_reader = csv.reader(filein, delimiter=',', quotechar='\"')\n",
    "    for row in file_reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV and Quote characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ['Name', 'Gender', 'Species', 'Appearance', 'Notable Phrase']\n",
      "5 ['Fozzie', 'Male', 'Bear', '1976', 'Wocka, Wokca!']\n",
      "5 ['Kermit', 'Male', 'Frog', '1955', \"It's not easy being green.\"]\n",
      "6 ['Piggy', 'Female', 'Pig', '1974', '“I don’t care what you think of me', ' unless you think I’m awesome. In which case you are right.”']\n",
      "5 ['Gonzo', 'Male', '', '1970', 'Weirdos have more fun.']\n",
      "6 ['Rowlf', 'Male', 'Dog', '1962', '“Boy', ' is this piano outta tune! I love outta tune pianos.”']\n",
      "5 ['Beaker', '', 'Muppet', '1977', 'Meep']\n",
      "5 ['Janice', 'Female', 'Muppet', '1975', 'Groovy, man']\n",
      "5 ['Hilda', 'Female', 'Muppet', '1976', \"Gonzo, aren't you a little old to carry around a teddy bear?\"]\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir / \"MuppetsTable_broken.csv\") as filein:\n",
    "    file_reader = csv.reader(filein)\n",
    "    for row in file_reader:\n",
    "        print(len(row),row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir / \"MuppetsTable_broken.csv\") as filein:\n",
    "    new_table = filein.read().replace('“','\"').replace('”','\"')\n",
    "\n",
    "    fileout = open(data_dir / \"MuppetsTable_fixed.csv\",'w')\n",
    "    fileout.write(new_table)\n",
    "    fileout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ['Name', 'Gender', 'Species', 'Appearance', 'Notable Phrase']\n",
      "5 ['Fozzie', 'Male', 'Bear', '1976', 'Wocka, Wokca!']\n",
      "5 ['Kermit', 'Male', 'Frog', '1955', \"It's not easy being green.\"]\n",
      "5 ['Piggy', 'Female', 'Pig', '1974', 'I don’t care what you think of me, unless you think I’m awesome. In which case you are right.']\n",
      "5 ['Gonzo', 'Male', '', '1970', 'Weirdos have more fun.']\n",
      "5 ['Rowlf', 'Male', 'Dog', '1962', 'Boy, is this piano outta tune! I love outta tune pianos.']\n",
      "5 ['Beaker', '', 'Muppet', '1977', 'Meep']\n",
      "5 ['Janice', 'Female', 'Muppet', '1975', 'Groovy, man']\n",
      "5 ['Hilda', 'Female', 'Muppet', '1976', \"Gonzo, aren't you a little old to carry around a teddy bear?\"]\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir / \"MuppetsTable_fixed.csv\") as filein:\n",
    "    file_reader = csv.reader(filein)\n",
    "    for row in file_reader:\n",
    "        print(len(row), row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fozzie 1976\n",
      "Kermit 1955\n",
      "Piggy 1974\n",
      "Gonzo 1970\n",
      "Rowlf 1962\n",
      "Beaker 1977\n",
      "Janice 1975\n",
      "Hilda 1976\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir / \"MuppetsTable_fixed.csv\") as filein:\n",
    "    reader = csv.DictReader(filein)\n",
    "    for row in reader:\n",
    "        print(row['Name'], row['Appearance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Pandas CSV reader: `read_csv()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir / \"MuppetsTable_fixed.csv\") \n",
    "display(df.iloc[:,:4]) # Using iloc to get first four columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectangular rich data: Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1069 entries, 0 to 1068\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Country Name   1066 non-null   object\n",
      " 1   Country Code   1064 non-null   object\n",
      " 2   Series Name    1064 non-null   object\n",
      " 3   Series Code    1064 non-null   object\n",
      " 4   2011 [YR2011]  1064 non-null   object\n",
      " 5   2012 [YR2012]  1064 non-null   object\n",
      " 6   2013 [YR2013]  1064 non-null   object\n",
      " 7   2014 [YR2014]  1064 non-null   object\n",
      " 8   2015 [YR2015]  1064 non-null   object\n",
      " 9   2016 [YR2016]  1064 non-null   object\n",
      " 10  2017 [YR2017]  1064 non-null   object\n",
      " 11  2018 [YR2018]  1064 non-null   object\n",
      " 12  2019 [YR2019]  1064 non-null   object\n",
      " 13  2020 [YR2020]  1064 non-null   object\n",
      "dtypes: object(14)\n",
      "memory usage: 117.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wb_df = pd.read_excel(data_dir / \"World Bank Indicators 2012-2021.xlsx\")\n",
    "\n",
    "display(wb_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(wb_df.iloc[:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_df.iloc[:,:4].tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the [:-5] is what slices off the last five rows\n",
    "wb_df.iloc[:-5].to_excel(data_dir / \"Cleaned_Popular_Indicators.xlsx\",\n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested data: `JSON` (JavaScript Object Notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "mdata = json.loads(open(data_dir / \"omdb_Muppet_search_page_1.json\").read())\n",
    "type(mdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Search\": [\n",
      "    {\n",
      "      \"Title\": \"The Muppet Christmas Carol\",\n",
      "      \"Year\": \"1992\",\n",
      "      \"imdbID\": \"tt0104940\",\n",
      "      \"Type\": \"movie\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"The Muppet Movie\",\n",
      "      \"Year\": \"1979\",\n",
      "      \"imdbID\": \"tt0079588\",\n",
      "      \"Type\": \"movie\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"The Muppet\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(mdata,indent=2)[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Search', 'totalResults', 'Response'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 True\n"
     ]
    }
   ],
   "source": [
    "print(mdata['totalResults'],\n",
    "      mdata['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'The Muppet Christmas Carol', 'Year': '1992', 'imdbID': 'tt0104940', 'Type': 'movie'}\n"
     ]
    }
   ],
   "source": [
    "print(mdata['Search'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Title\": \"The Muppet Christmas Carol\",\n",
      "  \"Year\": \"1992\",\n",
      "  \"imdbID\": \"tt0104940\",\n",
      "  \"Type\": \"movie\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(mdata['Search'][0],indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = { \"Search\":[\n",
    "                 {\"Title\":\"Ghosts of Hidden Valley\",\n",
    "                  \"Year\":2010},\n",
    "                 {\"Title\":\"The Perspex Event\",\n",
    "                  \"Year\":2018}]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(results[\"Search\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.json_normalize(mdata[\"Search\"])\n",
    "display(mdf.iloc[:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(mdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested markup languages: HTML and XML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML: Hypertext Markup Language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia as a data source "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia as HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091805\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir / \"Canada - Wikipedia.html\") as infile: \n",
    "    wiki_HTML = infile.read()\n",
    "\n",
    "print(len(wiki_HTML))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Canada - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakF\n"
     ]
    }
   ],
   "source": [
    "print(wiki_HTML[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup for markup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada - Wikipedia\n",
      "4381\n"
     ]
    }
   ],
   "source": [
    "soup = bs4.BeautifulSoup(wiki_HTML, 'html.parser')\n",
    "\n",
    "print(soup.title.text)\n",
    "\n",
    "# Query the soup for all 'a' tags. (Knowing that 'a' tags refer to links)\n",
    "links = soup.find_all(\"a\")\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scepticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head:  <a id=\"top\"></a>\n",
      "Tail:  <a href=\"https://www.mediawiki.org/\"><img alt=\"Powered by MediaWiki\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/poweredby_mediawiki_88x31.png\" srcset=\"/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x\" width=\"88\"/></a>\n",
      "Random:  <a href=\"/wiki/Toronto\" title=\"Toronto\">Toronto</a>\n"
     ]
    }
   ],
   "source": [
    "print(\"Head: \", links[0])\n",
    "print(\"Tail: \", links[-1])\n",
    "\n",
    "spot = random.choice(links)\n",
    "print(\"Random: \", spot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': 'https://www.mediawiki.org/'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[-1].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4377 'href' links in this file.\n",
      "The first 'href' link:\n",
      "<a href=\"/wiki/Wikipedia:Featured_articles\" title=\"This is a featured article. Click here for more information.\"><img alt=\"Featured article\" data-file-height=\"438\" data-file-width=\"462\" decoding=\"async\" height=\"19\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/30px-Cscr-featured.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/40px-Cscr-featured.svg.png 2x\" width=\"20\"/></a>\n"
     ]
    }
   ],
   "source": [
    "href_links = [x for x in soup.find_all('a') if 'href' in x.attrs]\n",
    "\n",
    "print(f\"There are {len(href_links)} 'href' links in this file.\")\n",
    "\n",
    "print(f\"The first 'href' link:\\n{href_links[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 950 'href' & '://' links in this file.\n",
      "The first 'href' and '://' link:\n",
      "https://en.wikipedia.org/w/index.php?title=Canada&action=edit\n"
     ]
    }
   ],
   "source": [
    "href_ext_links = [x for x in soup.find_all('a') if\n",
    "                  'href' in x.attrs and # Do this if clause first\n",
    "                  '://' in x.get('href')] # since this depends on first if\n",
    "\n",
    "print(f\"There are {len(href_ext_links)} 'href' & '://' links in this file.\")\n",
    "print(f\"The first 'href' and '://' link:\\n{href_ext_links[0].get('href')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3427 'href' internal links in this file.\n",
      "The first 'href' internal link:\n",
      "/wiki/Wikipedia:Featured_articles\n"
     ]
    }
   ],
   "source": [
    "href_int_links = [x for x in soup.find_all('a')\n",
    "                  if 'href' in x.attrs and \"://\" not in x.get('href')]\n",
    "\n",
    "print(f\"There are {len(href_int_links)} 'href' internal links in this file.\")\n",
    "print(f\"The first 'href' internal link:\\n{href_int_links[0].get('href')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277240\n",
      "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\n",
      "  <siteinfo>\n",
      "    <sitename>Wikipedia</sitename>\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir / \"Canada - Wikipedia Special Export.xml\") as infile: \n",
    "    wiki_XML = infile.read()\n",
    "\n",
    "print(len(wiki_XML))\n",
    "print(wiki_XML[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079679373\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "wiki_XML = wiki_XML.replace('<text xml:space=','<wikitext xml:space=')\n",
    "\n",
    "soup = bs4.BeautifulSoup(wiki_XML, 'lxml')\n",
    "\n",
    "print(soup.mediawiki.page.revision.id.text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "parentid\n",
      "timestamp\n",
      "contributor\n",
      "minor\n",
      "comment\n",
      "model\n",
      "format\n",
      "text\n",
      "sha1\n"
     ]
    }
   ],
   "source": [
    "# for i in soup.children: print(i.name)\n",
    "# for i in soup.html.children: print(i.name)\n",
    "# for i in soup.html.body.children: print(i.name)\n",
    "# for i in soup.html.body.mediawiki.children: print(i.name) \n",
    "# for i in soup.html.body.mediawiki.page: print(i.name)\n",
    "for i in soup.html.body.mediawiki.page.revision: \n",
    "    if name := i.name: print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<id>1079679373</id>\n",
      "1079679373\n"
     ]
    }
   ],
   "source": [
    "print(soup.html.body.mediawiki.page.revision.id)\n",
    "# Notice how we can shorten it (and get the text directly): \n",
    "print(soup.revision.id.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<id>5042916</id>\n",
      "page\n"
     ]
    }
   ],
   "source": [
    "print(soup.id, soup.id.parent.name, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `xmltodict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to install xmltodict.\n",
    "# This code is extra careful to check for the right version of Python for\n",
    "# installation. In fairness, `pip install <library>` usually works  fine.\n",
    "\n",
    "try: \n",
    "    import xmltodict\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install xmltodict\n",
    "    import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir / \"Canada - Wikipedia Special Export.xml\") as infile:\n",
    "    doc = xmltodict.parse(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['mediawiki'])\n",
      "odict_keys(['@xmlns', '@xmlns:xsi', '@xsi:schemaLocation', '@version', '@xml:lang', 'siteinfo', 'page'])\n",
      "http://www.mediawiki.org/xml/export-0.10/\n"
     ]
    }
   ],
   "source": [
    "print(doc.keys())\n",
    "print(doc['mediawiki'].keys())\n",
    "print(doc['mediawiki']['@xmlns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079679373\n"
     ]
    }
   ],
   "source": [
    "print(doc['mediawiki']['page']['revision']['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'RevisionID': 1079679373, 'PageID': 5042916}, 'Other Data', 3.1415]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "data_example = {'RevisionID':1079679373, 'PageID':5042916}\n",
    "data_for_pickle = [data_example,'Other Data',3.1415]\n",
    "\n",
    "pickle.dump(data_for_pickle,\n",
    "            open(data_dir / 'temp.pkl','wb'))\n",
    "\n",
    "# Check to see if the data comes back as we expected\n",
    "data_from_pkl = pickle.load(open(data_dir / 'temp.pkl','rb'))\n",
    "print(data_from_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long term storage: Pickles and feather "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending and reflecting "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
