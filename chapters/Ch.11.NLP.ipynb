{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Introducing Natural Language Processing \n",
    "\n",
    "Thank you for checking out the code for: \n",
    "\n",
    "> Hogan, Bernie (2023) _From Social Science to Data Science_. London, UK: Sage Publications. \n",
    "\n",
    "\n",
    "Last updated: 24 October 2023 \n",
    "\n",
    "This notebook contains the code from the book, along with the headers and additional author notes that are not in the book as a way to help navigate the code. You can run this notebook in a browser by clicking the buttons below. \n",
    "    \n",
    "The version that is uploaded to GitHub should have all the results pasted, but the best way to follow along is to clear all outputs and then start afresh. To do this in Jupyter go the menu and select \"Kernel -> Restart Kernel and Clear all Outputs...\". To do this on Google Colab go to the menu and select \"Edit -> Clear all outputs\".\n",
    "    \n",
    "The most up-to-date version of this code can be found at https://www.github.com/berniehogan/fsstds \n",
    "\n",
    "Additional resources and teaching materials can be found on Sage's website for this book. \n",
    "\n",
    "All code for the book and derivative code on the book's repository is released open source under the  MIT license. \n",
    "    \n",
    "\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/berniehogan/fsstds/main?filepath=chapters%2FCh.11.NLP.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/berniehogan/fsstds/blob/main/chapters/Ch.11.NLP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key definitions in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x1b'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 97\n"
     ]
    }
   ],
   "source": [
    "print(\"a\",ord(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~"
     ]
    }
   ],
   "source": [
    "# 0 through 32 do not show up below or are whitespace, so they are omitted\n",
    "for i in range(33,127): print(chr(i),end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Text to Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "\n",
    "stack = \"movies\"\n",
    "data_dir = Path().cwd().parent / \"data\" / f\"{stack}.stackexchange.com\"\n",
    "\n",
    "# pickle_file = data_dir / \"movies.stackexchange.com\" / \"movies.pkl\"\n",
    "\n",
    "# if pickle_file.exists():\n",
    "#     stack_df = pickle.load(open(pickle_file ,'rb'))\n",
    "#     print(stack_df.shape)\n",
    "# else:\n",
    "#     print(\"Please download and clean the Stack_df data as done in Chapter 10\",\n",
    "#           \"The data is available from\",\n",
    "#           \"https://archive.org/download/stackexchange\")\n",
    "\n",
    "stack_df = pd.read_feather(data_dir / \"Posts.feather\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sample simple NLP workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was       2\n",
       "the       2\n",
       "of        2\n",
       "It        1\n",
       "best      1\n",
       "times,    1\n",
       "it        1\n",
       "worst     1\n",
       "times     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"It was the best of times, it was the worst of times\"\n",
    "\n",
    "pd.Series(s.split(\" \")).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was      2\n",
       "the      2\n",
       "of       2\n",
       "times    2\n",
       "It       1\n",
       "best     1\n",
       ",        1\n",
       "it       1\n",
       "worst    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wordpunct_tokenize(s)\n",
    "pd.Series(result).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/berniehogan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "times    2\n",
       "best     1\n",
       "worst    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "result_filtered = [word for word in result\n",
    "                   if (word.lower() not in stopWords) and word.isalpha()]\n",
    "\n",
    "pd.Series(result_filtered).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offic       3\n",
       "run         2\n",
       "ran         2\n",
       "financ      2\n",
       "campaign    1\n",
       "huge        1\n",
       "tab         1\n",
       "victori     1\n",
       "parti       1\n",
       "offici      1\n",
       "troubl      1\n",
       "away        1\n",
       "state       1\n",
       "good        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''After running for office, his campaign office ran a huge tab \n",
    "for the victory party. But with finances officially in trouble, his \n",
    " finance officer ran away stating \"it was a good run\"!'''\n",
    "\n",
    "tokens = wordpunct_tokenize(text)\n",
    "\n",
    "tokens_filtered = [word.lower() for word in tokens\n",
    "                   if (word.lower() not in stopWords) and word.isalpha()]\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemlist = [porter.stem(t) for t in tokens_filtered]\n",
    "\n",
    "pd.Series(stemlist).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/berniehogan/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/berniehogan/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_26871\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_26871_level0_col0\" class=\"col_heading level0 col0\" >Word</th>\n",
       "      <th id=\"T_26871_level0_col1\" class=\"col_heading level0 col1\" >Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row0_col0\" class=\"data row0 col0\" >running</td>\n",
       "      <td id=\"T_26871_row0_col1\" class=\"data row0 col1\" >run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row1_col0\" class=\"data row1 col0\" >office</td>\n",
       "      <td id=\"T_26871_row1_col1\" class=\"data row1 col1\" >office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row2_col0\" class=\"data row2 col0\" >campaign</td>\n",
       "      <td id=\"T_26871_row2_col1\" class=\"data row2 col1\" >campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row3_col0\" class=\"data row3 col0\" >office</td>\n",
       "      <td id=\"T_26871_row3_col1\" class=\"data row3 col1\" >office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row4_col0\" class=\"data row4 col0\" >ran</td>\n",
       "      <td id=\"T_26871_row4_col1\" class=\"data row4 col1\" >run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row5_col0\" class=\"data row5 col0\" >huge</td>\n",
       "      <td id=\"T_26871_row5_col1\" class=\"data row5 col1\" >huge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row6_col0\" class=\"data row6 col0\" >tab</td>\n",
       "      <td id=\"T_26871_row6_col1\" class=\"data row6 col1\" >tab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row7_col0\" class=\"data row7 col0\" >victory</td>\n",
       "      <td id=\"T_26871_row7_col1\" class=\"data row7 col1\" >victory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row8_col0\" class=\"data row8 col0\" >party</td>\n",
       "      <td id=\"T_26871_row8_col1\" class=\"data row8 col1\" >party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row9_col0\" class=\"data row9 col0\" >finances</td>\n",
       "      <td id=\"T_26871_row9_col1\" class=\"data row9 col1\" >finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row10_col0\" class=\"data row10 col0\" >officially</td>\n",
       "      <td id=\"T_26871_row10_col1\" class=\"data row10 col1\" >officially</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_26871_row11_col0\" class=\"data row11 col0\" >trouble</td>\n",
       "      <td id=\"T_26871_row11_col1\" class=\"data row11 col1\" >trouble</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x166243990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wlemma = WordNetLemmatizer()\n",
    "\n",
    "lemmadf = pd.DataFrame(tokens_filtered,columns=[\"Word\"])\n",
    "lemmadf['Lemma'] = lemmadf[\"Word\"].map(\n",
    "                       lambda word: wlemma.lemmatize(word, pos=\"v\"))\n",
    "lemmadf[:12].style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Approaches to analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring documents with sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/berniehogan/anaconda3/lib/python3.11/site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/berniehogan/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/berniehogan/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/berniehogan/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/berniehogan/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (2023.7.22)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import vaderSentiment\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install vaderSentiment\n",
    "    import vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow I'm feeling great today, the weather is lovely. HOORAY!!!\n",
      "{'neg': 0.0, 'neu': 0.216, 'pos': 0.784, 'compound': 0.959}\n",
      "\n",
      "I would be lying if I said this was a really bad day\n",
      "{'neg': 0.395, 'neu': 0.605, 'pos': 0.0, 'compound': -0.8016}\n",
      "\n",
      "I would not be lying if I said this was a really bad day\n",
      "{'neg': 0.204, 'neu': 0.646, 'pos': 0.149, 'compound': -0.254}\n",
      "\n",
      "Hey bro! Those sick beats brought down the house!\n",
      "{'neg': 0.327, 'neu': 0.673, 'pos': 0.0, 'compound': -0.5972}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "texts = [\"Wow I'm feeling great today, the weather is lovely. HOORAY!!!\",\n",
    "         \"I would be lying if I said this was a really bad day\",\n",
    "         \"I would not be lying if I said this was a really bad day\",\n",
    "         \"Hey bro! Those sick beats brought down the house!\"]\n",
    "\n",
    "for i in texts:\n",
    "    print(i, analyzer.polarity_scores(i), sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BodyText</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7791</th>\n",
       "      <td>This is all just speculation, but some points ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5443</th>\n",
       "      <td>Initially it was a intended to be a drama, but...</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31224</th>\n",
       "      <td>I am currently doing an analysis on the movie ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38188</th>\n",
       "      <td>John tried to face the detectives and tried to...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55442</th>\n",
       "      <td>Does Revolutions XX00 mean the RPM of the pro...</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                BodyText    pos    neg\n",
       "7791   This is all just speculation, but some points ...  0.000  0.051\n",
       "5443   Initially it was a intended to be a drama, but...  0.110  0.123\n",
       "31224  I am currently doing an analysis on the movie ...  0.000  0.000\n",
       "38188  John tried to face the detectives and tried to...  0.000  0.000\n",
       "55442   Does Revolutions XX00 mean the RPM of the pro...  0.036  0.024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_df = stack_df.sample(2000,random_state=12345).copy()\n",
    "\n",
    "senti_sr = sample_df[\"BodyText\"].map(\n",
    "    lambda text: analyzer.polarity_scores(text))\n",
    "\n",
    "senti_df = pd.json_normalize(senti_sr)\n",
    "senti_df.index = senti_sr.index\n",
    "\n",
    "sample_df = pd.concat([sample_df, senti_df],axis=1)\n",
    "\n",
    "display(sample_df[[\"BodyText\",\"pos\",\"neg\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting keywords: TF-IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As a convenience, all the imports needed for this function\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text, lower_case = True, stop_words = True, \n",
    "               lemma = True, join_words = \" \"):\n",
    "\n",
    "    if lower_case: text = text.lower()\n",
    "\n",
    "    tokens = [x for x in wordpunct_tokenize(text) if x.isalpha()]\n",
    "\n",
    "    if stop_words: \n",
    "        tokens = [word for word in tokens \n",
    "                  if word not in stopwords.words('english')]\n",
    "    if lemma: \n",
    "        tokens = [WordNetLemmatizer().lemmatize(word,pos='v') \n",
    "                  for word in tokens]\n",
    "\n",
    "    if join_words:    return join_words.join(tokens)\n",
    "    else:             return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.85 s, sys: 1.87 s, total: 8.71 s\n",
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Multiprocessing doesn't really help here unfortunately\n",
    "\n",
    "documents = sample_df[\"BodyText\"].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "wordmat = vectorizer.fit_transform(documents).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the video - some things to check \n",
    "\n",
    "# print(dir(vectorizer))\n",
    "\n",
    "# print(wordmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18169896 0.10037503 1.94813427 ... 0.14068523 0.22302795 0.23354509]]\n",
      "[0.18169896 0.10037503 1.94813427 ... 0.14068523 0.22302795 0.23354509]\n"
     ]
    }
   ],
   "source": [
    "tfidf_scores = wordmat.sum(axis=0)\n",
    "\n",
    "print(tfidf_scores)\n",
    "\n",
    "print(tfidf_scores.A1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie    39.532631\n",
       "film     39.447474\n",
       "would    33.622581\n",
       "one      33.169395\n",
       "time     30.221115\n",
       "show     30.110691\n",
       "know     30.110662\n",
       "see      29.650258\n",
       "make     29.310130\n",
       "say      27.825130\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_words = pd.Series(tfidf_scores.A1, index=word_list)\n",
    "\n",
    "display(top_words.sort_values(ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier as an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier,classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbc_df = stack_df[(stack_df[\"PostTypeId\"]=='1') |\n",
    "                  (stack_df[\"PostTypeId\"]=='2')] \\\n",
    "                 [[\"PostTypeId\",\"BodyText\"]][:2000]\n",
    "\n",
    "nbc_df[\"CleanTokens\"] = nbc_df[\"BodyText\"].map(\n",
    "                         lambda x: clean_text(x,join_words=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counter(nbc_df[\"CleanTokens\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12748 words in total. 3262 appear at least 6 times.\n"
     ]
    }
   ],
   "source": [
    "min_count = 6\n",
    "\n",
    "all_words =  pd.Series(Counter(nbc_df[\"CleanTokens\"].sum()))\n",
    "# all_words =  pd.Series(nbc_df[\"CleanTokens\"].sum()).value_counts()\n",
    "\n",
    "\n",
    "print(f\"{len(all_words)} words in total.\",\n",
    "      f\"{len(all_words[all_words >= min_count])} appear at least\",\n",
    "      f\"{min_count} times.\")\n",
    "\n",
    "above_min_words = frozenset(all_words[all_words >= min_count].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostTypeId\n",
      "2    1192\n",
      "1     808\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features = nbc_df[\"CleanTokens\"].map(lambda x: \n",
    "                {i:i in set(x) for i in above_min_words})\n",
    "\n",
    "print(nbc_df[\"PostTypeId\"].value_counts())\n",
    "feature_list = list(zip(features, nbc_df[\"PostTypeId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 0.666\n",
      "Most Informative Features\n",
      "                    wiki = True                2 : 1      =      8.1 : 1.0\n",
      "                 anybody = True                1 : 2      =      7.2 : 1.0\n",
      "               excellent = True                2 : 1      =      6.3 : 1.0\n",
      "                    tone = True                2 : 1      =      6.3 : 1.0\n",
      "                 disease = True                1 : 2      =      6.2 : 1.0\n",
      "None\n",
      "CPU times: user 2.65 s, sys: 19.8 ms, total: 2.67 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_set, test_set = feature_list[:1000], feature_list[1000:]\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(f\"The accuracy of the classifier is {classify.accuracy(classifier, test_set):0.3f}\")\n",
    "print(classifier.show_most_informative_features(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions and reflections "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
