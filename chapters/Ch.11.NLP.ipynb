{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading language: Encoding text \n",
    "Thank you for checking out the code for: \n",
    "\n",
    "> Hogan, Bernie (2022, forthcoming) _From Social Science to Data Science_. Sage Publications. \n",
    "\n",
    "This notebook contains the code from the book, along with the headers and additional author notes that are not in the book as a way to help navigate the code. You can run this notebook in a browser by clicking the buttons below. \n",
    "    \n",
    "The version that is uploaded to GitHub should have all the results pasted, but the best way to follow along is to clear all outputs and then start afresh. To do this in Jupyter go the menu and select \"Kernel -> Restart Kernel and Clear all Outputs...\". To do this on Google Colab go to the menu and select \"Edit -> Clear all outputs\".\n",
    "    \n",
    "The most up-to-date version of this code can be found at https://www.github.com/berniehogan/fsstds \n",
    "\n",
    "Additional resources and teaching materials can be found on Sage's forthcoming website for this book. \n",
    "\n",
    "All code for the book and derivative code on the book's repository is released open source under the  MIT license. \n",
    "    \n",
    "\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/berniehogan/fsstds/main?filepath=chapters%2FCh.11.NLP.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/berniehogan/fsstds/blob/main/chapters/Ch.11.NLP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key definitions in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x1b'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 97\n"
     ]
    }
   ],
   "source": [
    "print(\"a\",ord(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~"
     ]
    }
   ],
   "source": [
    "# 0 through 32 do not show up below or are whitespace, so they are omitted\n",
    "for i in range(33,127): print(chr(i),end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Text to Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61184, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path().cwd().parent / \"data\"\n",
    "pickle_file = data_dir / \"movies_stack_df.pkl\"\n",
    "\n",
    "if pickle_file.exists():\n",
    "    stack_df = pickle.load(open(pickle_file ,'rb'))\n",
    "    print(stack_df.shape)\n",
    "else:\n",
    "    print(\"Please download and clean the Stack_df data as done in Chapter 10\",\n",
    "          \"The data is available from\",\n",
    "          \"https://archive.org/download/stackexchange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sample simple NLP workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was       2\n",
       "the       2\n",
       "of        2\n",
       "It        1\n",
       "best      1\n",
       "times,    1\n",
       "it        1\n",
       "worst     1\n",
       "times     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"It was the best of times, it was the worst of times\"\n",
    "\n",
    "pd.Series(s.split(\" \")).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was      2\n",
       "the      2\n",
       "of       2\n",
       "times    2\n",
       "It       1\n",
       "best     1\n",
       ",        1\n",
       "it       1\n",
       "worst    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wordpunct_tokenize(s)\n",
    "pd.Series(result).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "times    2\n",
       "best     1\n",
       "worst    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "result_filtered = [word for word in result\n",
    "                   if (word.lower() not in stopWords) and word.isalpha()]\n",
    "\n",
    "pd.Series(result_filtered).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offic       3\n",
       "run         2\n",
       "ran         2\n",
       "financ      2\n",
       "campaign    1\n",
       "huge        1\n",
       "tab         1\n",
       "victori     1\n",
       "parti       1\n",
       "offici      1\n",
       "troubl      1\n",
       "away        1\n",
       "state       1\n",
       "good        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''After running for office, his campaign office ran a huge tab \n",
    "for the victory party. But with finances officially in trouble, his \n",
    " finance officer ran away stating \"it was a good run\"!'''\n",
    "\n",
    "tokens = wordpunct_tokenize(text)\n",
    "\n",
    "tokens_filtered = [word.lower() for word in tokens\n",
    "                   if (word.lower() not in stopWords) and word.isalpha()]\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemlist = [porter.stem(t) for t in tokens_filtered]\n",
    "\n",
    "pd.Series(stemlist).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wlemma = WordNetLemmatizer()\n",
    "\n",
    "lemmadf = pd.DataFrame(tokens_filtered,columns=[\"Word\"])\n",
    "lemmadf['Lemma'] = lemmadf[\"Word\"].map(\n",
    "                       lambda word: wlemma.lemmatize(word, pos=\"v\"))\n",
    "lemmadf[:12].style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Approaches to analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring documents with sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import vaderSentiment\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install vaderSentiment\n",
    "    import vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow I'm feeling great today, the weather is lovely. HOORAY!!!\n",
      "{'neg': 0.0, 'neu': 0.216, 'pos': 0.784, 'compound': 0.959}\n",
      "\n",
      "I would be lying to if I said this was a really bad day\n",
      "{'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.8016}\n",
      "\n",
      "I would not be lying if I said this was a really bad day\n",
      "{'neg': 0.204, 'neu': 0.646, 'pos': 0.149, 'compound': -0.254}\n",
      "\n",
      "Hey bro! Those sick beats brought down the house!\n",
      "{'neg': 0.327, 'neu': 0.673, 'pos': 0.0, 'compound': -0.5972}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "texts = [\"Wow I'm feeling great today, the weather is lovely. HOORAY!!!\",\n",
    "         \"I would be lying to if I said this was a really bad day\",\n",
    "         \"I would not be lying if I said this was a really bad day\",\n",
    "         \"Hey bro! Those sick beats brought down the house!\"]\n",
    "\n",
    "for i in texts:\n",
    "    print(i, analyzer.polarity_scores(i), sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = stack_df.sample(2000,random_state=12345).copy()\n",
    "\n",
    "senti_sr = sample_df[\"CleanBody\"].map(\n",
    "    lambda text: analyzer.polarity_scores(text))\n",
    "\n",
    "senti_df = pd.json_normalize(senti_sr)\n",
    "senti_df.index = senti_sr.index\n",
    "\n",
    "sample_df = pd.concat([sample_df, senti_df],axis=1)\n",
    "\n",
    "display(sample_df[[\"CleanBody\",\"pos\",\"neg\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting keywords: TF-IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a convenience, all the imports needed for this function\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, lower_case = True, stop_words = True, \n",
    "               lemma = True, join_words = \" \"):\n",
    "\n",
    "    if lower_case: text = text.lower()\n",
    "\n",
    "    tokens = [x for x in wordpunct_tokenize(text) if x.isalpha()]\n",
    "\n",
    "    if stop_words: \n",
    "        tokens = [word for word in tokens \n",
    "                  if word not in stopwords.words('english')]\n",
    "    if lemma: \n",
    "        tokens = [WordNetLemmatizer().lemmatize(word,pos='v') \n",
    "                  for word in tokens]\n",
    "\n",
    "    if join_words:    return join_words.join(tokens)\n",
    "    else:             return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sample_df[\"CleanBody\"].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "wordmat = vectorizer.fit_transform(documents).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie    40.471016\n",
       "film     37.397752\n",
       "would    34.719049\n",
       "know     32.913460\n",
       "one      32.505876\n",
       "show     31.135298\n",
       "like     30.643401\n",
       "see      30.517230\n",
       "time     30.006641\n",
       "get      29.471909\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_scores = wordmat.sum(axis=0)\n",
    "\n",
    "wordlist = vectorizer.get_feature_names_out()\n",
    "top_words = pd.Series(tfidf_scores.A1, index=wordlist)\n",
    "\n",
    "top_words.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier as an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier,classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc_df = stack_df[(stack_df[\"PostTypeId\"]=='1') |\n",
    "                  (stack_df[\"PostTypeId\"]=='2')] \\\n",
    "                 [[\"PostTypeId\",\"CleanBody\"]][:2000]\n",
    "\n",
    "nbc_df[\"CleanTokens\"] = nbc_df[\"CleanBody\"].map(\n",
    "                         lambda x: clean_text(x,join_words=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12748 words in total. 3263 appear at least 6 times.\n"
     ]
    }
   ],
   "source": [
    "min_count = 6\n",
    "\n",
    "all_words =  pd.Series(Counter(nbc_df[\"CleanTokens\"].sum()))\n",
    "\n",
    "print(f\"{len(all_words)} words in total.\",\n",
    "      f\"{len(all_words[all_words >= min_count])} appear at least\",\n",
    "      f\"{min_count} times.\")\n",
    "\n",
    "above_min_words = frozenset(all_words[all_words >= min_count].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    1192\n",
      "1     808\n",
      "Name: PostTypeId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features = nbc_df[\"CleanTokens\"].map(lambda x: \n",
    "                {i:i in set(x) for i in above_min_words})\n",
    "\n",
    "print(nbc_df[\"PostTypeId\"].value_counts())\n",
    "feature_list = list(zip(features, nbc_df[\"PostTypeId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666\n",
      "Most Informative Features\n",
      "                    wiki = True                2 : 1      =      8.1 : 1.0\n",
      "                 anybody = True                1 : 2      =      7.2 : 1.0\n",
      "               excellent = True                2 : 1      =      6.3 : 1.0\n",
      "                    tone = True                2 : 1      =      6.3 : 1.0\n",
      "                 disease = True                1 : 2      =      6.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = feature_list[:1000], feature_list[1000:]\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions and reflections "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
